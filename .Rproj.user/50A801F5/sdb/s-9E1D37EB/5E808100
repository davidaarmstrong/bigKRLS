{
    "contents" : "#' Kernel Regularized Least Squares with Big Matrices\n#' \n#' @param y A vector of observations on the dependent variable; missing values not allowed \n#' @param X A matrix of observations of the independent variables; missing values not allowed \n#' @param lambda Regularization parameter. Default: estimated based (in part) on the eigenvalues of the kernel\n#' @param sigma Bandwidth, shorthand for sigma squared. Default: sigma <- ncol(X). Since x variables are standardized, faciltates interprepation of the kernel.\n#' @return bigKRLS object containing slope and uncertainty estimates\n#' @examples\n#'N <- 5000  # proceed with caution above N = 5,000\n#'k <- 4\n#'X <- matrix(rnorm(N), ncol=k)\n#'X <- cbind(X, sample(0:1, replace = T, size = nrow(X)))\n#'y <- X %*% runif(ncol(X)) + rnorm(N/k)\n#' out <- bigKRLS(X = X, y = y)\n#' @useDynLib bigKRLS\n#' @importFrom Rcpp evalCpp\n#' @import bigalgebra biganalytics bigmemory\n#' @export\nbigKRLS <- function (X = NULL, y = NULL, lambda = NULL, \n                          sigma = NULL, derivative = TRUE, binary = TRUE, vcov = TRUE, \n                          print.level = 3, L = NULL, U = NULL, tol = NULL, eigtrunc = NULL) \n{\n  # suppressing warnings from bigmatrix\n  oldw <- getOption(\"warn\")\n  options(warn = -1)\n  options(bigmemory.allow.dimnames=TRUE)\n  \n  if(print.level == 3){\n    print(\"starting KRLS...\")\n    timestamp()\n  }\n  \n  if(!is.big.matrix(X)){\n    X <- as.big.matrix(X, type='double')\n  } \n  \n  if(!is.big.matrix(y)){\n    y <- matrix(y, ncol=1)\n    y <- as.big.matrix(y, type='double')\n  } \n  \n  if (colsd(y) == 0) {\n    stop(\"y is a constant\")\n  }\n  \n  miss.ind <- colna(X)\n  if (sum(miss.ind) > 0) {\n    stop(paste(\"the following rows in X contain missing data, which must be removed:\", \n               paste((1:length(miss.ind))[miss.ind > 0], collapse = ', '), collapse=''))\n  }\n  if (colna(y) > 0) {\n    stop(\"y contains missing data.\") \n  }\n\n  if (!is.null(eigtrunc)) {\n    \n    if (!is.numeric(eigtrunc)) \n      stop(\"eigtrunc, if used, must be numeric\")\n    if (eigtrunc > 1 | eigtrunc < 0) \n      stop(\"eigtrunc must be between 0 and 1\")\n    if (eigtrunc == 0) {\n      eigtrunc = NULL\n      warning(\"eigtrunc of 0 equivalent to no eigen truncation\")\n    } \n  }\n  \n  n <- nrow(X)\n  d <- ncol(X)\n  \n  if (n != nrow(y)) {\n    stop(\"nrow(X) not equal to number of elements in y.\")\n  }\n  \n  stopifnot(is.logical(derivative), is.logical(vcov), is.logical(binary))\n  if (derivative == TRUE) {\n    if (vcov == FALSE) {\n      stop(\"derivative==TRUE requires vcov=TRUE\")\n    }\n  }\n  \n  if (is.null(sigma)) {     # default initialization \n    sigma <- d # sigma enters the kernel so as to make the scale make sense\n    # sigma actually shorthand for \"sigma squared\" \n  }else{\n    stopifnot(is.vector(sigma), length(sigma) == 1, is.numeric(sigma), \n              sigma > 0)\n  }\n  \n  X.init <- X\n  X.init.sd <- colsd(X)\n  X.init.range <- t(colrange(X))\n  \n  if (min(X.init.sd) == 0) {\n    stop(paste(\"the following columns in X are constant and must be removed:\",\n               which(X.init.sd == 0)))\n  }\n  \n  x.is.binary <- apply(X, 2, function(x){length(unique(x))}) == 2 \n  treat.x.as.binary <- matrix((x.is.binary + binary) == 2, nrow=1) # x is binary && user wants first differences\n  # to do: modify so that user may estimate derivatives of only some binaries...\n  colnames(treat.x.as.binary) <- colnames(X)\n  \n  if(sum(treat.x.as.binary) > 0){\n    print(paste(\"The following x variable(s) is (are) binary: \",\n                paste(colnames(X)[treat.x.as.binary], collapse=\", \"), \". \", \n                ifelse(binary, \n                       \"First differences will be computed for those binaries instead of local (pairwise) derivatives (default).\",\n                       \"Even for those binaries, local (pairwise) derivatives will be computed (not default).\"),\n                sep=\"\"))\n  }\n  \n  y.init <- deepcopy(y)\n  y.init.sd <- colsd(y)\n  y.init.mean <- colmean(y.init)\n  \n  for(i in 1:ncol(X)){\n    X[,i] <- (X[,i] - mean(X[,i]))/sd(X[,i])\n  }\n  y[,1] <- (y[,1] - mean(y[,1]))/sd(y[,1])\n  \n  if(print.level == 3){\n    print(\"done cleaning the data... getting Kernel...\")\n    timestamp()\n  }\n  \n  K <- NULL  # K is the kernel\n  \n  K <- bGaussKernel(X, sigma)\n  \n  if(print.level == 3){\n    print(\"got Kernel... getting Eigen\")\n    timestamp()\n  }\n  \n  Eigenobject <- bEigen(K) \n  \n  if(print.level == 3){\n    print(\"got Eigen... getting Lambda\")\n    timestamp()\n  }\n  \n  if (is.null(lambda)) {\n    noisy <- ifelse(print.level > 2, TRUE, FALSE)\n    lambda <- lambdasearch(L = L, U = U, y = y, Eigenobject = Eigenobject, \n                           eigtrunc = eigtrunc, noisy = noisy, cl=cl)\n    \n    if (print.level > 3) {\n      cat(\"Lambda that minimizes Leave-One-Out-Error (Loo) Loss is:\", \n          round(lambda, 5), \"\\n\")\n    }\n  }else {\n    stopifnot(is.vector(lambda), length(lambda) == 1, is.numeric(lambda), \n              lambda > 0)\n  }\n  \n  out <- solveforc(y = y, Eigenobject = Eigenobject, lambda = lambda, \n                   eigtrunc = eigtrunc, cl=cl)\n  \n  # solveforc obtains the vector of weights \n  # that assign importance to the similarity scores, found in K\n\n  yfitted <- K %*% as.matrix(out$coeffs, nrow=1)\n  \n  if(print.level == 3){\n    print(\"got coefficients, predicted values... getting vcovmatc\")\n    timestamp()\n  }\n  \n  if (vcov == TRUE) {\n    sigmasq <- (1/n) * bCrossProd(y - yfitted)[1,1]\n    \n    if (is.null(eigtrunc)) {  # default\n      m <- bMultDiag(Eigenobject$vectors, \n                     sigmasq * (Eigenobject$values + lambda)^-2)\n      vcovmatc <- bTCrossProd(m, Eigenobject$vectors)\n\n    }else{\n      \n      lastkeeper = max(which(Eigenobject$values >= eigtrunc * Eigenobject$values[1]))\n      \n      m <- bMultDiag(sub.big.matrix(Eigenobject$vectors, \n                                    firstCol=1, \n                                    lastCol=lastkeeper), \n                     sigmasq * (sub.big.matrix(Eigenobject$vectors, \n                                               firstCol=1, \n                                               lastCol=lastkeeper) + lambda)^-2)\n      vcovmatc <- bTCrossProd(m, sub.big.matrix(Eigenobject$vectors, \n                                                firstCol=1, \n                                                lastCol=lastkeeper))\n    }\n    # to do: add parameter to save eigen to disk \n    remove(Eigenobject)\n    gc()\n\n    vcovmatyhat <- bCrossProd(K, vcovmatc %*% K)\n  }else {\n    vcov.c <- NULL\n    vcov.fitted <- NULL\n  }\n  \n  if(print.level == 3){\n    print(\"got vcovmatc...\")\n    timestamp()\n  }  \n  \n  avgderiv <- varavgderivmat <- derivmat <- NULL\n  \n  if (derivative == TRUE) {\n    \n    derivmat <- big.matrix(nrow=n, ncol=d, init=NA)\n    varavgderivmat <- big.matrix(nrow=d, ncol=1, init=NA)\n    \n    print(\"getting derivatives....\")\n    \n    for(i in 1:d){\n      \n      if(treat.x.as.binary[i]){\n        \n        print(paste(\"Computing first differences for binary variable\", colnames(X)[i], \".\", sep=\"\"))\n        timestamp()\n        X.tmp <- big.matrix(nrow=nrow(X)*3, ncol=ncol(X), init=NA)\n        for(ind in 1:n){\n          X.tmp[ind,] <- X[ind,]\n          X.tmp[(ind + n),] <- X[ind,]\n          X.tmp[(ind + 2*n),] <- X[ind,]\n        }\n        \n        X.tmp[1:n, i] <-  X.init.range[2, i]        # max value on original scale, usually 1\n        X.tmp[(n+1):(2*n), i] <- X.init.range[1, i] # min value on original scale, usually 0\n        \n        print(\"getting temporary Kernel... (slow step)...\")\n        \n        # correct to rounding error, though there may be a faster way to do this\n        K.tmp <- bTempKernel(X.tmp, sigma)\n        \n        remove(X.tmp)\n        y.fitted.tmp <- K.tmp %*% as.matrix(out$coeffs)\n        \n        derivmat[,i] <- y.fitted.tmp[1:n,] - y.fitted.tmp[(n + 1):(2*n),]\n        remove(y.fitted.tmp)\n        \n        h <- as.big.matrix(matrix(rep(c(1/n, -(1/n)), each = n), ncol = 1))\n        vcov.fitted <- bTCrossProd(K.tmp %*% vcovmatc, K.tmp)\n        remove(K.tmp)\n        varavgderivmat[i] <- 2 * as.matrix((bCrossProd(h, vcov.fitted) %*% h))\n        remove(vcov.fitted)\n        remove(h)\n        gc()\n      }else{\n        \n        x <- as.matrix(X[,i])\n        distancek <- apply(x, 1, function(xi){x - xi})  # see if dist, lower.tri can expedite (symmetric)\n        remove(x)\n        \n        print(paste(\"L\", i, sep=\"\"))\n        timestamp()\n        L <- bElementwise(distancek, K)\n        remove(distancek)\n        gc()\n        \n        print(paste(\"computing local derivatives of\", colnames(X)[i], sep=\"\"))\n        timestamp()\n        \n        derivmat[,i] <- (-2/sigma) * as.matrix(L %*% as.big.matrix(out$coeff))\n        \n        print(paste(\"computing variance of local derivatives of\", colnames(X)[i]))\n        timestamp()\n        varavgderivmat[i] <- (1/n^2) * (-2/sigma)^2 * sum(bCrossProd(L, vcovmatc %*% L))\n        remove(L)\n        gc()\n      }\n    }\n    \n    if(print.level == 3){\n      print(\"rescaling and other odds and ends...\")\n      timestamp()\n    }\n    \n    derivmat <- y.init.sd * derivmat\n    for(i in 1:ncol(derivmat)){\n      derivmat[,i] <- derivmat[,i]/X.init.sd[i]\n    }\n    \n    attr(derivmat, \"scaled:scale\") <- NULL\n    colnames(derivmat) <- colnames(X)\n    avgderiv <- matrix(colmean(derivmat), nrow=1)\n    attr(avgderiv, \"scaled:scale\") <- NULL\n    varavgderivmat <- matrix((y.init.sd/X.init.sd)^2 * as.matrix(varavgderivmat), \n                             nrow=1)\n    colnames(varavgderivmat) <- colnames(X)\n    attr(varavgderivmat, \"scaled:scale\") <- NULL\n  }\n  \n  yfitted <- yfitted * y.init.sd + matrix(y.init.mean, \n                                             nrow=nrow(yfitted),\n                                             ncol=ncol(yfitted))\n  if (vcov == TRUE) {\n    vcov.c <- (y.init.sd^2) * vcovmatc\n    vcov.fitted <- (y.init.sd^2) * vcovmatyhat\n  }else {\n    vcov.c <- NULL\n    vcov.fitted <- NULL\n  }\n  Looe <- out$Le * y.init.sd\n  R2 <- 1 - (var(y.init - yfitted)/(y.init.sd^2))\n  z <- list(K = K, coeffs = out$coeffs, Looe = Looe, fitted = yfitted, \n            X = X.init, y = y.init, sigma = sigma, lambda = lambda, \n            R2 = R2, derivatives = derivmat, avgderivatives = avgderiv, \n            var.avgderivatives = varavgderivmat, vcov.c = vcov.c, \n            vcov.fitted = vcov.fitted, binaryindicator = treat.x.as.binary)\n  class(z) <- \"bigKRLS\" \n  \n  if (print.level > 0 && derivative == TRUE) {\n    output <- setNames(as.vector(z$avgderivatives), colnames(z$avgderivatives))\n    cat(\"\\n Average Marginal Effects:\\n \\n\")\n    print(output)\n    cat(\"\\n Percentiles of Local Derivatives:\\n \\n\")\n    print(apply(z$derivatives, 2, quantile, \n                probs = c(0.05, 0.25, 0.5, 0.75, 0.95)))\n    print(\"For more detail, use summary() on the outputted object.\")\n  }\n\n  return(z)\n  \n  options(warn = oldw)\n}  \n\nlambdasearch <- function (L = NULL, U = NULL, y = NULL, Eigenobject = NULL, tol = NULL, \n                          noisy = FALSE, eigtrunc = NULL, cl=cl){\n  n <- nrow(y)\n  if (is.null(tol)) {\n    tol <- 10^-3 * n \n  } else {\n    stopifnot(is.vector(tol), length(tol) == 1, is.numeric(tol), \n              tol > 0)\n  }\n  if (is.null(U)) {\n    U <- n\n    while (sum(Eigenobject$values/(Eigenobject$values + U)) < 1) {\n      U <- U - 1\n    }\n  } else {\n    stopifnot(is.vector(U), length(U) == 1, is.numeric(U), \n              U > 0)\n  }\n  if (is.null(L)) {\n    q <- which.min(abs((Eigenobject$values - max(Eigenobject$values)/1000)))\n\n    L = .Machine$double.eps\n    \n    # .Machine is a variable holding information on the numerical characteristics \n    # of the machine R is running on, \n    # such as the largest double or integer and the machine's precision.\n    \n    # double.eps is the smallest positive floating-point number x \n    # such that 1 + x != 1. Normally 2.220446e-16.\n    \n    while (sum(Eigenobject$values/(Eigenobject$values + L)) > q) {\n      L <- L + 0.05 # L starts near 0 so sum(Eigenobject$values/(Eigenobject$values + L)) \n      # ~ sum(Eigenobject$values/Eigenobject$values) = N\n      # as L increases, the sum of the ratios decreases quickly\n      # since jumping from L = 0 to L = 0.05 leapfrogs many small eigenvalues...\n    } \n  } else {\n    stopifnot(is.vector(L), length(L) == 1, is.numeric(L), \n              L >= 0)\n  }\n  X1 <- L + (0.381966) * (U - L)\n  X2 <- U - (0.381966) * (U - L)\n\n  # looloss is Leave One Out Error Loss\n  \n  S1 <- looloss(lambda = X1, y = y, Eigenobject = Eigenobject, \n                eigtrunc = eigtrunc, cl=cl)\n  S2 <- looloss(lambda = X2, y = y, Eigenobject = Eigenobject, \n                eigtrunc = eigtrunc, cl=cl)\n  if (noisy) {\n    cat(\"L:\", L, \"X1:\", X1, \"X2:\", X2, \"U:\", U, \"S1:\", S1, \n        \"S2:\", S2, \"\\n\")\n  }\n  while (abs(S1 - S2) > tol) {\n    if (S1 < S2) {\n      U <- X2\n      X2 <- X1\n      X1 <- L + (0.381966) * (U - L)\n      S2 <- S1\n      S1 <- looloss(lambda = X1, y = y, Eigenobject = Eigenobject, \n                    eigtrunc = eigtrunc, cl=cl)\n    }\n    else {\n      L <- X1\n      X1 <- X2\n      X2 <- U - (0.381966) * (U - L)\n      S1 <- S2\n      S2 <- looloss(lambda = X2, y = y, Eigenobject = Eigenobject, \n                    eigtrunc = eigtrunc, cl=cl)\n    }\n    if (noisy) {\n      cat(\"L:\", L, \"X1:\", X1, \"X2:\", X2, \"U:\", U, \"S1:\", \n          S1, \"S2:\", S2, \"\\n\")\n    }\n  }\n  out <- ifelse(S1 < S2, X1, X2)\n  if (noisy) {\n    cat(\"Lambda:\", out, \"\\n\")\n  }\n  return(invisible(out))\n}\n\nsolveforc <- function (y = NULL, Eigenobject = NULL, lambda = NULL, eigtrunc = NULL, cl=cl) {\n  nn <- nrow(y)\n  if (is.null(eigtrunc)) {\n    #split this line into two separate operations\n    #the multdiag() call takes a trivial amount of time, so no need to parallelize\n    \n    m <- bMultDiag(Eigenobject$vectors, \n                   1/(Eigenobject$values + lambda))\n    Ginv <- bTCrossProd(m, Eigenobject$vectors)\n    \n  }else {\n    lastkeeper = max(which(Eigenobject$values >= eigtrunc * \n                             Eigenobject$values[1]))\n    lastkeeper = max(1, lastkeeper)\n    \n    m <- bMultDiag(Eigenobject$vectors,\n                   1/(Eigenobject$values[1:lastkeeper] + lambda),\n                   (1:lastkeeper))\n    Ginv <- bTCrossProd(m, Eigenobject$vectors, 1:lastkeeper)\n  }\n  \n  coeffs <- (Ginv %*% y)[,]\n  Le <- crossprod(coeffs/bDiag(Ginv))\n  return(list(coeffs = coeffs, Le = Le))\n}\n\nlooloss <- function (y = NULL, Eigenobject = NULL, lambda = NULL, eigtrunc = NULL, cl=cl) \n{\n  return(solveforc(y = y, Eigenobject = Eigenobject, lambda = lambda, \n                   eigtrunc = eigtrunc, cl=cl)$Le)\n} # not sure that there's any point to this function\n# could just make \"looloss\" mode a parameter of solveforc\n\n#' @export\npredict.bigKRLS <- function (object, newdata, se.fit = FALSE, ...) \n{\n  if (class(object) != \"bigKRLS\") {\n    warning(\"Object not of class 'bigKRLS'\")\n    UseMethod(\"predict\")\n    return(invisible(NULL))\n  }\n  if (se.fit == TRUE) {\n    if (is.null(object$vcov.c)) {\n      stop(\"recompute bigKRLS object with bigKRLS(,vcov=TRUE) to compute standart errors\")\n    }\n  }\n  newdata <- as.big.matrix(newdata)\n  if (ncol(object$X) != ncol(newdata)) {\n    stop(\"ncol(newdata) differs from ncol(X) from fitted krls object\")\n  }\n  Xmeans <- colmean(object$X)\n  Xsd <- colsd(object$X)\n  \n  for(i in 1:ncol(X)){\n    X[,i] <- (X[,i] - mean(X[,i]))/sd(X[,i])\n  }  \n  \n  newdata.init <- newdata\n  \n  for(i in 1:ncol(newdata)){\n    newdata[,i] <- (newdata[,i] - mean(newdata[,i]))/sd(newdata[,i])\n  }\n  \n  nn <- nrow(newdata)\n  newdata.X <- big.matrix(nrow=(nn + nrow(X)), \n                          ncol=(ncol(X)),\n                          init=NA)\n  for(i in 1:nn){\n    newdata.X[,i] <- newdata[,i]\n  }\n  for(j in (nn+1):(nn+nrow(X))){\n    newdata.X[,j] <- X[,j]\n  }\n  \n  \n  newdataK <- bGaussKernel(newdata.X, object$sigma)\n  newdataK <- sub.big.matrix(newdataK, \n                             firstRow = 1, lastRow = nn,\n                             firstCol = nn, lastCol=nrow(X))\n  \n  yfitted <- newdataK %*% as.matrix(object$coeffs, nrow=1)\n  if (se.fit) {\n    vcov.c.raw <- object$vcov.c * as.vector((1/var(object$y)))\n    vcov.fitted <- bTCrossProd(newdataK %*% vcov.c.raw, newdataK)\n    vcov.fit <- var(object$y) * vcov.fitted\n    se.fit <- matrix(sqrt(diag(vcov.fit)), ncol = 1)\n  }\n  else {\n    vcov.fit <- se.fit <- NULL\n  }\n  yfitted <- (yfitted * sd(object$y) + mean(object$y))\n  return(list(fit = yfitted, se.fit = se.fit, vcov.fit = vcov.fit, \n              newdata = newdata, newdataK = newdataK))\n}\n\n#' @export\nsummary.bigKRLS <- function (object, \n                             probs = c(0.05, 0.25, 0.5, 0.75, 0.95), ...) \n{\n  if (class(object) != \"bigKRLS\") {\n    warning(\"Object not of class 'bigKRLS'\")\n    UseMethod(\"summary\")\n    return(invisible(NULL))\n  }\n  cat(\"* *********************** *\\n\")\n  cat(\"Model Summary:\\n\\n\")\n  cat(\"R2:\", object$R2, \"\\n\\n\")\n  d <- ncol(object$X)\n  n <- nrow(object$X)\n  coefficients <- matrix(NA, d, 0)\n  rownames(coefficients) <- colnames(object$X)\n  if (is.null(object$derivatives)) {\n    cat(\"\\n\")\n    cat(\"recompute krls object with krls(...,derivative = TRUE) to get summary of marginal effects\\n\")\n    return(invisible(NULL))\n  }\n  est <- t(object$avgderivatives)\n  se <- sqrt(t(object$var.avgderivatives))\n  tval <- est/se\n  avgcoefficients <- cbind(est, se, tval, 2 * pt(abs(tval), \n                                                 n - d, lower.tail = FALSE))\n  colnames(avgcoefficients) <- c(\"Est\", \"Std. Error\", \"t value\", \n                                 \"Pr(>|t|)\")\n  if (sum(object$binaryindicator) > 0) {\n    rownames(avgcoefficients)[object$binaryindicator] <- paste(rownames(avgcoefficients)[object$binaryindicator], \n                                                               \"*\", sep = \"\")\n  }\n  cat(\"Average Marginal Effects:\\n\")\n  print(avgcoefficients, ...)\n  if (sum(object$binaryindicator) > 0) {\n    cat(\"\\n(*) average dy/dx is for discrete change of dummy variable from min to max (i.e. usually 0 to 1))\\n\\n\")\n  }\n  qderiv <- apply(object$derivatives, 2, quantile, probs = probs)\n  if (sum(object$binaryindicator) > 0) {\n    colnames(qderiv)[object$binaryindicator] <- paste(colnames(qderiv)[object$binaryindicator], \n                                                      \"*\", sep = \"\")\n  }\n  qderiv <- t(qderiv)\n  cat(\"\\n\")\n  cat(\"Percentiles of Local Derivatives:\\n\")\n  print(qderiv)\n  if (sum(object$binaryindicator) > 0) {\n    cat(\"\\n(*) quantiles of dy/dx is for discrete change of dummy variable from min to max (i.e. usually 0 to 1))\\n\\n\")\n  }\n  ans <- list(coefficients = avgcoefficients, \n              qcoefficients = qderiv)\n  class(ans) <- \"summary.bigKRLS\"\n  return(invisible(ans))\n}\n\n##################\n# Rccp Functions #\n##################\n\nbMultDiag <- function (X, d) {\n  #rcpp_multdiag.cpp\n  out <- big.matrix(nrow=nrow(X),\n                    ncol=ncol(X),\n                    init=0,\n                    type='double')\n  d <- as.big.matrix(matrix(d, nrow=1))\n  \n  BigMultDiag(X@address, d@address, out@address)\n  \n  return(out)\n}\n\n#' @export\nbEigen <- function(X){\n  #rcpp_eigen.cpp\n  vals <- big.matrix(nrow = 1,\n                     ncol = ncol(X),\n                     init = 0,\n                     type = 'double')\n  vecs <- big.matrix(nrow = nrow(X),\n                     ncol = ncol(X),\n                     init=0,\n                     type='double')\n  BigEigen(X@address, vals@address, vecs@address)\n  return(list('values' = vals[,], 'vectors' = vecs))\n}\n\n#' @export\nbGaussKernel <- function(X, sigma){\n  #rcpp_gauss_kernel.cpp\n  out <- big.matrix(nrow=nrow(X), ncol=nrow(X), init=0)\n  s <- big.matrix(1,1,type='double', init=sigma)\n  \n  BigGaussKernel(X@address, out@address, s@address)\n  return(out)\n}\n\n#' @export\nbTempKernel <- function(X, sigma){\n  #rcpp_temp_kernel.cpp\n  n <- nrow(X)/3\n  out <- big.matrix(nrow=(n*2), ncol=n, init=0)\n  s <- big.matrix(1,1,type='double', init=sigma)\n  \n  BigTempKernel(X@address, out@address, s@address)\n  return(out)\n}\n\n#' @export\nbCrossProd <- function(X,Y=NULL){\n  if(is.null(Y)){\n    Y <- deepcopy(X)\n  }\n  out <- big.matrix(nrow = ncol(X),\n                    ncol = ncol(Y),\n                    init = 0,\n                    type = 'double')\n  BigCrossProd(X@address, Y@address, out@address)\n  return(out)\n}\n\n#' @export\nbTCrossProd <- function(X,Y=NULL){\n  if(is.null(Y)){\n    Y <- deepcopy(X)\n  }\n  out <- big.matrix(nrow = nrow(X),\n                    ncol = nrow(Y),\n                    init = 0,\n                    type = 'double')\n  BigTCrossProd(X@address, Y@address, out@address)\n  return(out)\n}\n\n#' @export\nbDiag <- function(X){\n  # return the diagonal elements of a bigmatrix\n  out <- sapply(1:nrow(X), function(i){X[i,i]})\n  return(out)\n}\n\n#' @export\nbElementwise <- function(X,Y=NULL){\n  if(!is.big.matrix(X)){\n    X <- as.big.matrix(X)\n  }\n  if(is.null(Y)){\n    Y <- deepcopy(X)\n  }\n  \n  out <- big.matrix(nrow=nrow(X), ncol=ncol(X), init=0)\n  \n  BigElementwise(X@address, Y@address, out@address)\n  \n  return(out)\n}",
    "created" : 1463771406608.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4009295491",
    "id" : "5E808100",
    "lastKnownWriteTime" : 1465410638,
    "path" : "~/Dropbox/bigKRLS/R/bigKRLS.R",
    "project_path" : "R/bigKRLS.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}